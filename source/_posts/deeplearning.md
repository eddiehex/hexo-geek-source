---
title: 深度学习入门：从 MLP 到 RNN/LSTM，核心架构解析
date: 2025-04-18
categories:
  - deeplearning
tags: [deeplearning]
toc: true
excerpt: "深度学习概念"
---

深度学习是机器学习领域一个非常强大且活跃的分支，它主要依赖于**人工神经网络**，特别是那些拥有**多层**结构的神经网络，来从数据中学习复杂的模式。理解不同的神经网络架构是入门深度学习的关键一步，因为不同的架构擅长处理不同类型的数据和任务。

今天，我们将回顾以下几种核心架构：

1. **MLP (多层感知机)：** 最基础的前馈网络。
2. **CNN (卷积神经网络)：** 图像处理的利器。
3. **RNN (循环神经网络) 及 LSTM：** 处理序列数据的能手。

## 1. MLP (Multilayer Perceptron): 深度学习的基石

MLP，即多层感知机，是最基础、最经典的一种**前馈神经网络**。你可以将它视为更复杂网络的构建块。

**架构特点：**

- 由**输入层**、一个或多个**隐藏层**和**输出层**组成。
- 层与层之间是**全连接 (Fully Connected)** 的：上一层的每一个神经元节点都连接到下一层的每一个神经元节点。

**处理过程（前向传播）：**

1. **输入层：** 接收原始数据。**输入层的节点数量**等于你**单个样本的特征数量**。每个节点代表样本的一个特征值。
2. **隐藏层/输出层：** 数据从上一层流向当前层。在当前层的每个节点，进行以下两个关键步骤：
   - **线性转换 (Linear Transformation)：** 接收来自上一层所有节点的输出，将它们分别乘以对应的连接**权重 (Weights)**，然后求和，并加上该节点的**偏置 (Bias)**。这步计算是线性的 (Wx + b)。
   - **非线性激活 (Non-linear Activation)：** 将线性转换的结果通过一个**非线性激活函数**（如 ReLU）。**这是非常关键的一步！**它引入了非线性，使得多层网络能够学习复杂的非线性关系，而不是仅仅叠加多个线性转换（那样最终仍是线性）。
3. 这个“线性转换 + 非线性激活”的过程在每个隐藏层重复进行，直到数据到达输出层，产生最终预测结果。

**总结：** MLP 通过多层**非线性**转换，将输入数据映射到输出空间。它是一种通用模型，但处理高维、有结构的数据（如图像）效率不高。

## 2. CNN (Convolutional Neural Network): 图像世界的专家

CNN，即卷积神经网络，是专门为处理具有**网格结构**的数据而设计的，最成功的应用领域是**图像处理**。它通过模仿人类视觉系统的一些特性来工作。

**架构特点：**

CNN 的核心在于其特殊的层：

- **卷积层 (Convolutional Layer)：** CNN 的灵魂。
- **池化层 (Pooling Layer)：** 常用于降维。
- **全连接层 (Fully Connected Layer)：** 通常在网络的末端用于分类。

**处理过程：**

1. **输入层：** 接收图像数据。图像是 3D 的（高度、宽度、通道数）。
2. **卷积层：**
   - 使用小的**滤波器 (Filter) / 卷积核 (Kernel)** 在输入图片（或上一层的特征图）上**滑动**。
   - 滤波器内包含学习到的权重，它与覆盖区域的像素值进行卷积运算（乘加）。
   - 每个滤波器检测一种特定的**局部特征**（如边缘、纹理）。
   - **参数共享：** 同一个滤波器在图片不同位置重复使用，大大减少参数。
   - **局部连接：** 每个神经元只连接到输入的一个小区域。
   - 输出是一系列**特征图 (Feature Map)**，每个特征图由一个滤波器生成，表示该特征在输入中的分布。
3. **激活函数：** 对卷积层的输出（特征图）应用非线性激活。
4. **池化层 (可选)：**
   - 在特征图上滑动窗口，取窗口内的最大值（最大池化）或平均值（平均池化）。
   - **作用：** 减小特征图的空间尺寸（降维），减少计算量，并增加对特征位置微小变化的鲁棒性。
5. **重复：** 通常会堆叠多个卷积层和池化层。随着网络加深，学习到的特征越来越抽象和复杂。
6. **展平层 (Flatten Layer)：** 在将 3D 的特征图输入到 MLP 式的全连接层之前，需要将其展平为一个 1D 向量。
7. **全连接层 & 输出层：** 接收展平后的特征向量，进行 MLP 式的计算，最终输出分类或回归结果。

**总结：** CNN 利用卷积和池化操作，有效地捕捉图像的**空间局部性**和**特征的层次结构**，并通过参数共享提高了效率，使其成为图像处理领域的首选架构。

## 3. RNN (Recurrent Neural Network) & LSTM: 序列数据的记忆者

RNN，即循环神经网络，是为处理**序列数据**而设计的，比如文本、时间序列、语音等，这些数据点之间存在**顺序**和**依赖关系**。

**RNN 核心思想：记忆**

与 MLP 和 CNN 不同，RNN 具有**“记忆”**能力。它在处理序列中的当前数据点时，会考虑之前处理过的数据点的信息。这种记忆体现在其内部的**隐藏状态 (Hidden State)** 中，隐藏状态会在处理序列的过程中不断更新，并将信息从上一个时间步传递到下一个时间步，形成一个**循环**。

**处理过程：**

1. **输入：** 序列数据，一个数据点（如一个词的向量）在**一个时间步 (Time Step)** 输入到 RNN 单元。
2. **RNN 单元计算：** 在每个时间步 't'，RNN 单元接收：
   - 当前输入 x_t。
   - **上一个时间步的隐藏状态** h_{t-1}（这就是记忆的来源）。
   - 它将 x_t 和 h_{t-1} 结合（通过加权求和和激活函数）进行计算。
3. **输出：** 产生**当前时间步的隐藏状态** h_t（新的记忆）和可选的当前时间步的输出 y_t。
4. **循环：** 计算出的 h_t 会被作为**下一个时间步**的 h_{t-1} 输入到同一个 RNN 单元中。

**RNN 的挑战：长距离依赖**

标准 RNN 在处理**长序列**时，由于梯度消失等问题，很难有效地捕捉和利用序列中**长距离的依赖关系**。早期的信息在多次传递和计算后容易丢失。

**LSTM (Long Short-Term Memory): 增强的记忆**

LSTM 是 RNN 的一个重要改进，专门为了解决标准 RNN 的长距离依赖问题而设计。

**LSTM 的核心：细胞状态 (Cell State) 和 门 (Gates)**

LSTM 引入了一个独立的**细胞状态 (Cell State)**，它像一条信息高速公路，可以相对完整地传递信息。同时，它有三个特殊的**“门”**来精确控制信息的流动：

1. **遗忘门 (Forget Gate)：** 控制从**上一个细胞状态**中**丢弃**多少信息。
2. **输入门 (Input Gate)：** 控制将当前输入和隐藏状态中的**哪些新信息**添加到**当前细胞状态**中。
3. **输出门 (Output Gate)：** 控制从**当前细胞状态**中**输出**多少信息作为当前的**隐藏状态**。

通过这些门的协同作用，LSTM 能够**选择性地记忆**重要的信息，**遗忘**不重要的信息，从而在长序列中有效地保持和利用长期记忆。

**总结：** RNN（包括 LSTM）通过引入循环连接和隐藏状态，使得网络能够处理序列数据并具备记忆能力。LSTM 通过细胞状态和门控机制，克服了标准 RNN 在长序列上的局限性，是处理文本、语音等序列数据的强大工具。GRU 是 LSTM 的一个简化变体。

**结语**

MLP、CNN 和 RNN/LSTM 是深度学习中最基本也是最重要的几种神经网络架构。它们各自有不同的结构特点和适用场景：

- **MLP：** 通用基础，理解线性+非线性转换。
- **CNN：** 擅长网格数据（图像），理解卷积、池化和空间层次特征。
- **RNN/LSTM：** 擅长序列数据，理解循环、隐藏状态和长期记忆。

